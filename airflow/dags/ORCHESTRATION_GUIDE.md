# ğŸ¤– Bentley Budget Bot - DAG Orchestration Architecture

## ğŸ“Š Complete Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BENTLEY BUDGET BOT PIPELINE                    â”‚
â”‚                  Airbyte â†’ KNIME â†’ MLflow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Master DAG      â”‚  â† Coordinates entire pipeline
â”‚  (Manual/Daily)  â”‚     Triggers and monitors all stages
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Dataset: mysql://mansa_bot/binance_ohlcv
â”‚  1. Airbyte      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sync DAG        â”‚                                  â”‚
â”‚  (@hourly)       â”‚  Ingests external data          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ Writes to MySQL              â”‚
                                                      â”‚
                                                      â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Dataset: mysql://mansa_bot/knime_processed
                    â”‚  2. KNIME        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  CLI Workflow    â”‚                                  â”‚
                    â”‚  (Dataset)       â”‚  Processes data                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ Transforms & Cleans          â”‚
                                                                          â”‚
                                                                          â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  3. MLflow       â”‚
                                        â”‚  Logging DAG     â”‚
                                        â”‚  (Dataset)       â”‚  Tracks experiments
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â†’ Logs metrics
```

## ğŸ”— DAG Connections via Datasets

### Orchestration Flow:
1. **Airbyte Sync DAG** (`airbyte_sync_dag`)
   - **Schedule**: `@hourly`
   - **Produces**: `Dataset("mysql://mansa_bot/binance_ohlcv")`
   - **Action**: Fetches data from external sources â†’ MySQL

2. **KNIME CLI Workflow** (`knime_cli_workflow`)
   - **Schedule**: `[airbyte_dataset]` (triggered by Airbyte)
   - **Consumes**: `Dataset("mysql://mansa_bot/binance_ohlcv")`
   - **Produces**: `Dataset("mysql://mansa_bot/knime_processed")`
   - **Action**: Processes raw data â†’ Transformed data

3. **MLflow Logging DAG** (`mlflow_logging_dag`)
   - **Schedule**: `[airbyte_dataset, knime_dataset]` (triggered by both)
   - **Consumes**: Both Airbyte & KNIME datasets
   - **Action**: Logs metrics, tracks experiments

4. **Master Orchestration DAG** (`bentley_master_orchestration`)
   - **Schedule**: `@daily` (or manual trigger)
   - **Action**: Coordinates and monitors all pipeline stages

## ğŸ“‹ DAG Inventory

| DAG Name | Status | Owner | Schedule | Purpose |
|----------|--------|-------|----------|---------|
| `airbyte_sync_dag` | âœ… Active | airflow | @hourly | Data ingestion from Airbyte |
| `knime_cli_workflow` | âœ… Active | airflow | Dataset | Data processing via KNIME |
| `mlflow_logging_dag` | âœ… Active | bentley-bot | Dataset | Experiment tracking |
| `bentley_master_orchestration` | âœ… NEW | bentley-bot | @daily | Pipeline coordination |
| `bentleybot_dag` | âœ… Active | airflow | Manual | Trading logic |
| `stock_ingestion_pipeline` | âœ… Active | winston | @hourly | Multi-source stock data |
| `tiingo_data_pull` | âœ… Active | airflow | @daily | Tiingo financial data |

## ğŸ¯ Dataset-Driven Architecture

### What are Datasets?
Airflow **Datasets** enable data-aware scheduling where DAGs automatically trigger when their input data is ready. This creates a truly event-driven pipeline.

### Benefits:
- âœ… **Automatic orchestration** - No manual triggers needed
- âœ… **Data lineage** - Track data flow through pipeline
- âœ… **Decoupled DAGs** - Each DAG is independent
- âœ… **Event-driven** - React to data availability

## ğŸš€ Getting Started

### 1. Enable DAGs in Airflow UI
```bash
# All DAGs start in paused state
# In Airflow UI (http://localhost:8080):
# Toggle the switch next to each DAG to unpause
```

### 2. Verify Dataset Connections
```bash
# In Airflow UI, click on any DAG
# Navigate to "Graph" tab
# Look for dataset connections (shown as orange boxes)
```

### 3. Trigger Master Orchestration
```bash
# Option 1: Via Airflow UI
# Go to DAGs â†’ bentley_master_orchestration â†’ Play button

# Option 2: Via CLI
docker exec bentley-airflow-scheduler airflow dags trigger bentley_master_orchestration

# Option 3: Via VS Code Airflow Extension
# Right-click on bentley_master_orchestration â†’ Trigger DAG
```

### 4. Monitor Pipeline Execution
```bash
# View in Airflow UI:
http://localhost:8080/dags/bentley_master_orchestration/grid

# Or use VS Code Airflow Extension:
# - View DAG runs
# - Check task logs
# - Monitor progress
```

## ğŸ“Š VS Code Airflow Extension Features

Once connected, you can:

1. **View DAG Graph** - Visualize task dependencies
2. **Trigger DAGs** - Start pipeline with one click
3. **View Logs** - Debug task execution
4. **Monitor Status** - Real-time DAG status
5. **Browse Datasets** - See dataset lineage

### Access in VS Code:
- Click Airflow icon in sidebar
- Or: `Ctrl+Shift+P` â†’ "Airflow: Open"

## ğŸ”§ Configuration Status

| Component | Status | Details |
|-----------|--------|---------|
| Airflow Webserver | âœ… Running | Port 8080 |
| Airflow Scheduler | âœ… Running | Processing DAGs |
| MySQL Database | âœ… Running | Port 3307 |
| MLflow Server | âœ… Running | Port 5000 |
| VS Code Extension | âœ… Configured | admin/admin |
| DAG Folder | âœ… Mounted | ./dags |
| Dataset Orchestration | âœ… Enabled | 3 datasets defined |

## ğŸ“– Next Steps

### To make orchestration operational:

1. **Unpause all DAGs** in Airflow UI
2. **Configure Airbyte** connection ID in `Airbyt_sync_DAG_3.py`
3. **Configure KNIME** path in `Dag_Knime_CLI.py`
4. **Test individual DAGs** before full orchestration
5. **Monitor execution** via VS Code extension or Airflow UI

### Recommended Testing Order:

```bash
# 1. Test Airbyte sync first
docker exec bentley-airflow-scheduler airflow dags trigger airbyte_sync_dag

# 2. Verify KNIME triggers automatically (via dataset)
# Check in Airflow UI after ~1 minute

# 3. Verify MLflow triggers automatically (via dataset)
# Check in Airflow UI after KNIME completes

# 4. Run full orchestration
docker exec bentley-airflow-scheduler airflow dags trigger bentley_master_orchestration
```

## ğŸ“ Learning Resources

- [Airflow Datasets Documentation](https://airflow.apache.org/docs/apache-airflow/stable/concepts/datasets.html)
- [Data-Aware Scheduling](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/datasets.html)
- [DAG Dependencies](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)

---

**Status**: âœ… Orchestration configured and ready to use!  
**Last Updated**: November 23, 2025
